batch_size: 16
epoch_num: 10
seed: 954
patience: 5
lr: 0.00005
soft_verbalizer_lr_1: 0.00005
soft_verbalizer_lr_2: 0.0001
warmup_ratio: 0.1
max_len: 512
weight_decay: 0.015
cl_loss_weight: 0.8
csa_weight: 0.2
cl_tau: 0.1
kd_tau: 3.0

alpha_1_beta_1: 1.00
alpha_1_beta_n_1: 0.90
alpha_n_1_beta_1: 0.05
alpha_n_1_beta_n_1: 0.00

ce_label_smoothing: 0.01

dataset: Laptop
train_data_path: data/Laptops_Train_implicit_labeled.seg
test_data_path: data/Laptops_Test_implicit_labeled.seg
llm_generated_path: data/paraphrased_data/laptop_paraphrased_sentences.txt

# the dir of the pre-trained bert-base model
plm_dir: # your path

# the dir of saved checkpoints
output_dir: ./saved_checkpoints

input_template_text: '{"placeholder":"text_a"} [SEP] The {"placeholder":"text_b"} is {"mask"} .'
additional_template_text: '{"meta":"llm_generated_text"} [SEP] The {"placeholder":"text_b"} is {"mask"} .'